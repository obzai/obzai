{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cloud Logging Tutorial: Classification of Natural Images\n",
    "\n",
    "In this tutorial, we will guide you step-by-step through using the Obz AI python package to monitor and explain a machine learning model that classifies natural images to object labels. \n",
    "\n",
    "For anomaly detection, we utilize a data inspection routine based on Gaussian Mixture Model (GMM) fitted on First Order Image Features. \n",
    "For explainable AI, we compute attention maps and class-discriminative attention maps (CDAMs).\n",
    "\n",
    "**What will you learn?**\n",
    "\n",
    "- How to apply a simple image classification model using sample medical images.\n",
    "- How to log directly to the Obz AI cloud platform from your code.\n",
    "\n",
    "First let's install `obzai` package if you haven't already installed it. Also let's install `gdown` to download pretrained weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install obzai\n",
    "%pip install -q gdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the necessary modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import v2\n",
    "from torch import nn\n",
    "import torch\n",
    "import gdown\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you get started logging and visualizing data with the `ObzClient` from our Python library, you will need your unique API key. This key is required to securely connect your code to your account on our SaaS platform.\n",
    "\n",
    "**Here's how to find your API key:**\n",
    "\n",
    "1. **Log in to your Dashboard:**  \n",
    "   Open your web browser and go to [Obz.AI](http://app.obz.ai/). Log in using your registered email and password.\n",
    "\n",
    "2. **Navigate to the Settings section:**  \n",
    "   After logging in, look for a menu item called \"Settings\". Click on it to access your API settings. Make sure to copy this key now — you will need it in the next steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Security is important. Do not share your API token with anyone.\n",
    "API_TOKEN:str = \"your-api-token\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you can monitor or visualize the workings of your model using our library, you first need to define the model you want to track.\n",
    "\n",
    "In this tutorial, we will use a Vision Transformer (ViT) model that has been previously trained and fine-tuned on a subset of the ImageNet data called *Imagenette*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights already exist at tuned_models/imagenette_dino_s8.pth. Skipping download.\n"
     ]
    }
   ],
   "source": [
    "def download_weights(url, output_dir, filename):\n",
    "    \"\"\"\n",
    "    Downloads weights from the given URL if they are not already downloaded.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    output_path = os.path.join(output_dir, filename)\n",
    "    \n",
    "    if not os.path.exists(output_path):\n",
    "        print(f\"Downloading weights to {output_path}...\")\n",
    "        gdown.download(url, output_path)\n",
    "    else:\n",
    "        print(f\"Weights already exist at {output_path}. Skipping download.\")\n",
    "\n",
    "url = \"https://drive.google.com/uc?id=1zV21_xkjk6YamJ6Yrj6d5u1FzxC-oY37\"\n",
    "output_dir = \"tuned_models\"\n",
    "filename = \"imagenette_dino_s8.pth\"\n",
    "download_weights(url, output_dir, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure the ViT classifer based on DINO backbone\n",
    "\n",
    "We are adding a binary classification head (see how ```torch.nn.Linear```) onto a DINO backbone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTConfig, ViTModel\n",
    "\n",
    "class DINO(nn.Module):\n",
    "    \"\"\"\n",
    "    DINO Transformer model based on Huggingface implementation.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Backbone\n",
    "        config = ViTConfig.from_pretrained('facebook/dino-vits8', attn_implementation=\"eager\") # We propose eager implementation to return att scores gracefully.\n",
    "        self.backbone = ViTModel(config)\n",
    "        # Classfication head\n",
    "        self.head = torch.nn.Linear(384, 10)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, output_attentions:bool=False):\n",
    "        out = self.backbone(x, output_attentions=output_attentions)\n",
    "        x = out[\"pooler_output\"]\n",
    "        x = self.head(x)\n",
    "        if output_attentions:\n",
    "            att = out[\"attentions\"]\n",
    "            return x, att\n",
    "        else:\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "WEIGHTS_PATH = \"./tuned_models/imagenette_dino_s8.pth\"\n",
    "\n",
    "MODEL = DINO()\n",
    "MODEL.load_state_dict(torch.load(WEIGHTS_PATH, weights_only=True, map_location=torch.device(DEVICE)))\n",
    "MODEL = MODEL.to(DEVICE).eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's prepare the datasets. In general, you will need two separate sets of images:\n",
    "\n",
    "**Reference Dataset**: This dataset will be used to extract reference image features and to fit the outlier detection models. In ML, this may be a training dataset.\n",
    "\n",
    "**Inference Dataset**: This dataset will be treated as incoming new data on which you want to perform outlier detection. In ML, this may be testing or validation dataset or any new samples in production.\n",
    "\n",
    "For the tutorial case we will use ImageNet subset called: **Imagenette**. This dataset is readily available via **torchvision** package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import Imagenette\n",
    "\n",
    "# Transforms\n",
    "TRANSFORMS = v2.Compose([v2.ToImage(), \n",
    "                         v2.ToDtype(torch.float32, scale=True), \n",
    "                         v2.CenterCrop(size=(160,160)),\n",
    "                         v2.Resize(size=(224,224))])\n",
    "NORMALIZE = v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "# Datasets\n",
    "train_ds = Imagenette(root=\"./example_data\", split='train', size='160px', transform=TRANSFORMS, download=False)\n",
    "ref_set = torch.utils.data.Subset(train_ds, indices=[i for i in range(0, len(train_ds), 20)])\n",
    "val_ds = Imagenette(root=\"./example_data\", split='val', size='160px', transform=TRANSFORMS)\n",
    "\n",
    "# DataLoaders\n",
    "ref_loader = DataLoader(ref_set, batch_size=32, shuffle=False)\n",
    "inf_loader = DataLoader(val_ds, batch_size=6, shuffle=True)\n",
    "\n",
    "# Labels mapping\n",
    "CLASS_NAMES = [\"tench\", \"English springer\", \"cassette player\", \n",
    "               \"chain saw\", \"church\", \"French horn\", \"garbage truck\", \n",
    "               \"gas pump\", \"golf ball\", \"parachute\"]\n",
    "\n",
    "LOGIT2NAME = {\n",
    "    0: \"tench\",\n",
    "    1: \"English springer\",\n",
    "    2: \"cassette player\",\n",
    "    3: \"chain saw\",\n",
    "    4: \"church\",\n",
    "    5: \"French horn\",\n",
    "    6: \"garbage truck\",\n",
    "    7: \"gas pump\",\n",
    "    8: \"golf ball\",\n",
    "    9: \"parachute\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using ObzClient to Explain and Monitor a AI Model\n",
    "\n",
    "In this section, we'll show you how to set up and use the `ObzClient` to automatically log your data features and model explanation outputs to the Obz AI cloud platform.\n",
    "\n",
    "### Local Usage\n",
    "\n",
    "You can use the **Data Inspector Module** and **XAI Module** locally on your machine. To explore all the core features of these modules, please see the Jupyter notebook tutorials provided:\n",
    "- [data_inspector_tutorial.ipynb](./data_inspector_tutorial.ipynb)\n",
    "- [xai_tools_tutorial.ipynb](./xai_tools_tutorial.ipynb)\n",
    "\n",
    "### Cloud Logging with ObzClient\n",
    "\n",
    "To take full advantage of Obz AI, you can log your data features and model explanation outputs to the Obz AI cloud. This enables you to:\n",
    "- Track and organize input features and outlier statistics.\n",
    "- Store and visualize model explanations and results.\n",
    "- Collaborate and share insights via the Obz dashboard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Instantiate the Modules\n",
    "\n",
    "Instantiate and fit an Outlier Detector from the `Data Inspector` Module for outlier detection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features with FirstOrderExtractor: 100%|██████████| 15/15 [00:25<00:00,  1.72s/batch]\n"
     ]
    }
   ],
   "source": [
    "# Setup OutlierDetector\n",
    "from obzai.data_inspector.extractor import FirstOrderExtractor\n",
    "from obzai.data_inspector.detector import GMMDetector\n",
    "\n",
    "# Choose desired feature extractor. Chosen extractor will be used for monitoring.\n",
    "first_order_extrc = FirstOrderExtractor()\n",
    "\n",
    "# Pass choosen extractor(s) to chosen OutlierDetector. Below we utilize outlier detector based on Gaussian Mixture Models.\n",
    "gmm_detector = GMMDetector(extractors=[first_order_extrc], n_components=3, outlier_quantile=0.01, show_progress=True)\n",
    "# Call .fit() method with passed reference dataloader. \n",
    "# Method will extract desired image features and fit outlier detection model (in that case GMM).\n",
    "gmm_detector.fit(ref_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate your chosen explainability method from the `XAI Tool` Module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup XAI Tools\n",
    "from obzai.xai.xai_tool import CDAM, AttentionMap\n",
    "\n",
    "# Choose desired XAI Tools\n",
    "cdam_tool = CDAM(model=MODEL, \n",
    "                 mode='vanilla',                      # CDAM mode\n",
    "                 gradient_type=\"from_probabilities\",  # Whether backpropagate gradients from logits or probabilities.\n",
    "                 gradient_reduction=\"average\",        # Gradient reduction method.\n",
    "                 activation_type=\"softmax\")           # Activation function applied on logits. (Needed when gradients are backpropagated from probabilities.)\n",
    "# In CDAM you need to specify on which layer you want to create hooks.\n",
    "cdam_tool.create_hooks(layer_name=\"backbone.encoder.layer.11.layernorm_before\")\n",
    "\n",
    "attention_tool = AttentionMap(model=MODEL,\n",
    "                              attention_layer_id = -1,# ID of an attention layer from which to extract attention weights\n",
    "                              head = None             # ID of attention head to choose. If None, attention scores are averaged.\n",
    "                              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Prepare `ObzClient` for a cloud logging\n",
    "\n",
    "Now, when you have your fitted `OutlierDetector` instance and `XAITool` instances, you are ready to wrap it into your `ObzClient`!  \n",
    "It will take care of logging data to your cloud dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from obzai.client.obz_client import ObzClient\n",
    "\n",
    "client = ObzClient(detector=gmm_detector, \n",
    "                     xai_tools=[cdam_tool, attention_tool],\n",
    "                     verbose=True\n",
    "                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Authenticate `ObzClient` \n",
    "\n",
    "To use our library and send logs from your project to your online dashboard, you first need to authenticate your client. Authentication links your logs to your specific account and project, ensuring that you can securely access and manage your data.\n",
    "\n",
    "To authenticate, you will need an API key. This key is a unique identifier for your account and allows the library to send information to your dashboard. You can find your API key by logging into your account and visiting your dashboard. If you haven't registered yet, please create an account and follow the instructions to generate your API key.\n",
    "\n",
    "Once you have the API key, pass it to the client’s `.login()` method in your Python code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] Login successful\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entry for 'obz-backend' already exists in .netrc. Skipping.\n"
     ]
    }
   ],
   "source": [
    "# Login into client\n",
    "client.login(api_key=API_TOKEN, relogin=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you log in successfully, your credentials are automatically stored in a hidden `.netrc` file on your computer. This means you **do not have to enter your API key again** when using the `ObzClient`, authentication will happen automatically in future sessions.\n",
    "\n",
    "Now let's initialize a project with `.init_project()` method. This method accepts following arguments:  \n",
    "\n",
    "- **`project_name`** *(str, required)*: the name of the project you want to work with.  \n",
    "  - If the project already exists, you will be connected to it.\n",
    "  - If it is a new name, a new project will be created automatically.\n",
    "\n",
    "- **`ml_task`** *(str, required)*:  the type of machine learning task you are working on. We will use `multiclass_classification` in this tutorial.  \n",
    "  - This setting allows ObzClient to tailor the project’s dashboard and logging features to your specific task.\n",
    "\n",
    "- **`index2name`** *(dict, optional)*: A dictionary mapping numeric class indices to human-readable class names.  \n",
    "  - Providing this helps ObzClient visualize your results with actual class names, making your dashboard easier to understand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] Project initialized (ID=45).\n"
     ]
    }
   ],
   "source": [
    "# Project initialization\n",
    "client.init_project(project_name=\"imagenet_tutorial\",\n",
    "                    ml_task=\"multiclass_classification\",\n",
    "                    logit2name=LOGIT2NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Log data features and model explanations to the Obz AI cloud platform\n",
    "\n",
    "Now you can log your reference data. Simply call `.log_reference()` method on your client object. It will automatically send all reference data from your Outlier Detector into your Dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] Succesfully created ref entry.\n",
      "[INFO] Uploaded chunk 1/10 successfully.\n",
      "[INFO] Uploaded chunk 2/10 successfully.\n",
      "[INFO] Uploaded chunk 3/10 successfully.\n",
      "[INFO] Uploaded chunk 4/10 successfully.\n",
      "[INFO] Uploaded chunk 5/10 successfully.\n",
      "[INFO] Uploaded chunk 6/10 successfully.\n",
      "[INFO] Uploaded chunk 7/10 successfully.\n",
      "[INFO] Uploaded chunk 8/10 successfully.\n",
      "[INFO] Uploaded chunk 9/10 successfully.\n",
      "[INFO] Uploaded chunk 10/10 successfully.\n",
      "[INFO] All chunks uploaded successfully.\n"
     ]
    }
   ],
   "source": [
    "client.log_reference(reference_name=\"imagenet_tutorial_ref\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With `ObzClient` initiated with an outlier detector and an XAI tool, we are ready to `.run_and_log`, processing batches of samples in the inference data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] Uploaded batch of data from 6 images.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] Uploaded batch of data from 6 images.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] Uploaded batch of data from 6 images.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] Uploaded batch of data from 6 images.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] Uploaded batch of data from 6 images.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] Uploaded batch of data from 6 images.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:  6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] Uploaded batch of data from 6 images.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:  7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] Uploaded batch of data from 6 images.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:  8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] Uploaded batch of data from 6 images.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:  9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] Uploaded batch of data from 6 images.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:  10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] Uploaded batch of data from 6 images.\n"
     ]
    }
   ],
   "source": [
    "for idx, (image_batch, _) in enumerate(inf_loader):\n",
    "    if idx > 10:\n",
    "        break\n",
    "    \n",
    "    print(\"Batch: \", idx)\n",
    "\n",
    "    # STEP 0: Take a fresh batch of images:\n",
    "    image_batch, _ = next(iter(inf_loader))\n",
    "    image_batch = image_batch.to(DEVICE)\n",
    "\n",
    "    # STEP 1: Make inference:\n",
    "    res = client.run_and_log(model=MODEL, image_batch=image_batch, transform=NORMALIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
